{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hSntLTS7ZnIl",
   "metadata": {
    "id": "hSntLTS7ZnIl"
   },
   "source": [
    "# **Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JPaKwuIGtY3V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27852,
     "status": "ok",
     "timestamp": 1705729820000,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "JPaKwuIGtY3V",
    "outputId": "e97514fb-e99e-4974-c398-7cee2acae1de"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jYuZcRBnbBY8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9151,
     "status": "ok",
     "timestamp": 1705740838116,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "jYuZcRBnbBY8",
    "outputId": "cd33fb1a-3ad3-477c-b9a8-041f54fe5b70"
   },
   "outputs": [],
   "source": [
    "!pip install netCDF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7eeafd",
   "metadata": {
    "executionInfo": {
     "elapsed": 1459,
     "status": "ok",
     "timestamp": 1705729971075,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "ed7eeafd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from bs4 import BeautifulSoup\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import netCDF4\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2Ea7Khwwbaqi",
   "metadata": {
    "executionInfo": {
     "elapsed": 1099,
     "status": "ok",
     "timestamp": 1705729984608,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "2Ea7Khwwbaqi"
   },
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "import gc\n",
    "from keras import models, layers, metrics\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, Dense, Flatten, MaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras import losses\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xboQi5nBrzQD",
   "metadata": {
    "id": "xboQi5nBrzQD"
   },
   "source": [
    "# **Utilities Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U4BSbh8k4TsH",
   "metadata": {
    "id": "U4BSbh8k4TsH"
   },
   "source": [
    "This file downloads the satellite images which will then be processed\n",
    "\n",
    "Outline of this file:\n",
    "- Loops through each year and downloads .tar.gz files containing satellite images for Atlantic and Pacific hurricanes\n",
    "- Only extracts files from the .tar.gz files that contain images of hurricanes that we know the wind speed of\n",
    "- When script is finished, the Satellite Imagery folder contains all netcdf files, which hold the satellite images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pPRETKU94dNn",
   "metadata": {
    "id": "pPRETKU94dNn"
   },
   "source": [
    "Purpose: This builds and trains the neural network with the data processed. This file also validates the model, telling the user how accurate it is.\n",
    "\n",
    "Outline of this file:\n",
    "- Reads and augments images of hurricanes and their labels for use in convolutional neural network (CNN)\n",
    "- Builds a CNN and trains it on those images and labels\n",
    "- Validates the model using k-fold validation\n",
    "- Prints MAE and saves two graphs that show additional details about the model's error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Iy4vvVGE46xm",
   "metadata": {
    "id": "Iy4vvVGE46xm"
   },
   "source": [
    "Purpose of this file: This file builds and trains the neural network with the data processed by assemble.py. This file also validates the model, telling the user how accurate it is.\n",
    "\n",
    "Outline of this file:\n",
    "- Reads and augments images of hurricanes and their labels for use in convolutional neural network (CNN)\n",
    "- Builds a CNN and trains it on those images and labels\n",
    "- Validates the model using k-fold validation\n",
    "- Prints MAE and saves two graphs that show additional details about the model's error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FFBaBBw81Shr",
   "metadata": {
    "executionInfo": {
     "elapsed": 573,
     "status": "ok",
     "timestamp": 1705730048624,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "FFBaBBw81Shr"
   },
   "outputs": [],
   "source": [
    "def print_progress(action, progress, total):\n",
    "    percent_progress = round((progress / total) * 100, 1)\n",
    "    print('\\r' + action + '... ' + str(percent_progress) + '% (' + str(progress) + ' of ' + str(total) + ')', end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C7tgO0hSOJVs",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1705730050769,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "C7tgO0hSOJVs"
   },
   "outputs": [],
   "source": [
    "def read_and_prepare_data(validation_mode, k = 5, augment = True):\n",
    "    if validation_mode == 'k_fold':\n",
    "\n",
    "        # Read in data from files\n",
    "        images = np.load(image_path)\n",
    "        labels = np.load(label_path)\n",
    "\n",
    "        # Split the image and label datasets into k number of subsets\n",
    "        folded_images = []\n",
    "        folded_labels = []\n",
    "        for i in range(k):\n",
    "            start = int((i / k) * len(images))\n",
    "            end = int(((i + 1) / k) * len(images))\n",
    "            folded_images.append(images[start:end])\n",
    "            folded_labels.append(labels[start:end])\n",
    "\n",
    "        # Generate augmented images for each fold\n",
    "        folded_augmented_images = []\n",
    "        folded_augmented_labels = []\n",
    "        for i in range(k):\n",
    "            if augment:\n",
    "                print('\\nAugmenting Fold ' + str(i + 1) + ' of ' + str(k))\n",
    "                augmented_images, augmented_labels = augment_images(folded_images[i], folded_labels[i])\n",
    "                folded_augmented_images.append(augmented_images)\n",
    "                folded_augmented_labels.append(augmented_labels)\n",
    "\n",
    "        # Combine the folds into sets for each iteration of the model and standardize the data\n",
    "        train_images = []\n",
    "        train_labels = []\n",
    "        test_images = []\n",
    "        test_labels = []\n",
    "        for i in range(k):\n",
    "            train_images.append(np.concatenate(folded_images[:i] + folded_images[(i+1):]))\n",
    "            train_labels.append(np.concatenate(folded_labels[:i] + folded_labels[(i+1):]))\n",
    "            if augment:\n",
    "                train_images[i] = np.concatenate(([train_images[i]] + folded_augmented_images[:i] + folded_augmented_images[(i + 1):]))\n",
    "                train_labels[i] = np.concatenate(([train_labels[i]] + folded_augmented_labels[:i] + folded_augmented_labels[(i + 1):]))\n",
    "            test_images.append(folded_images[i])\n",
    "            test_labels.append(folded_labels[i])\n",
    "            train_images[i], test_images[i] = standardize_data(train_images[i], test_images[i])\n",
    "\n",
    "        return train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GHXasiXGN5_j",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1705730054633,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "GHXasiXGN5_j"
   },
   "outputs": [],
   "source": [
    "def augment_images(images, labels):\n",
    "    # Create generators to augment images\n",
    "    from keras.preprocessing import image\n",
    "    flip_generator = image.ImageDataGenerator(horizontal_flip = True, vertical_flip = True)\n",
    "    rotate_generator = image.ImageDataGenerator(rotation_range = 360, fill_mode = 'nearest')\n",
    "\n",
    "    # Accumulate augmented images and labels\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    # Loop each images in the set to augment\n",
    "    for i in range(len(images)):\n",
    "\n",
    "        # Reshape image for generator\n",
    "        image = np.reshape(images[i], (1, images[i].shape[0], images[i].shape[1], 1))\n",
    "        label = labels[i]\n",
    "\n",
    "        # Reset the number of augmented images have been created to zero\n",
    "        num_new_images = 0\n",
    "\n",
    "        # Generate 2 new images if the image is of a tropical cyclone between 50 and 75 knots\n",
    "        if 50 < label < 75:\n",
    "            for batch in flip_generator.flow(image, batch_size=1):\n",
    "                gc.collect()\n",
    "                new_image = np.reshape(batch[0], (batch[0].shape[0], batch[0].shape[1], 1))\n",
    "                augmented_images.append(new_image)\n",
    "                augmented_labels.append(label)\n",
    "                num_new_images += 1\n",
    "                if num_new_images == 2:\n",
    "                    break\n",
    "\n",
    "        # Generate 6 new images if the image is of a tropical cyclone between 75 and 100 knots\n",
    "        elif 75 < label < 100:\n",
    "            for batch in rotate_generator.flow(image, batch_size=1):\n",
    "                gc.collect()\n",
    "                new_image = np.reshape(batch[0], (batch[0].shape[0], batch[0].shape[1], 1))\n",
    "                augmented_images.append(new_image)\n",
    "                augmented_labels.append(label)\n",
    "                num_new_images += 1\n",
    "                if num_new_images == 6:\n",
    "                    break\n",
    "\n",
    "        # Generate 12 new images if the image is of a tropical cyclone greater than or equal to 100 knots\n",
    "        elif 100 <= label:\n",
    "            for batch in rotate_generator.flow(image, batch_size=1):\n",
    "                gc.collect()\n",
    "                new_image = np.reshape(batch[0], (batch[0].shape[0], batch[0].shape[1], 1))\n",
    "                augmented_images.append(new_image)\n",
    "                augmented_labels.append(label)\n",
    "                num_new_images += 1\n",
    "                if num_new_images == 12:\n",
    "                    break\n",
    "\n",
    "        print_progress('Augmenting Images', i + 1, len(images))\n",
    "\n",
    "    # Convert lists of images/labels into numpy arrays\n",
    "    augmented_images = np.array(augmented_images)\n",
    "    augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "    return augmented_images, augmented_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jUOuWKrCK3FD",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1705730079940,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "jUOuWKrCK3FD"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (50, 50, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation = 'relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation = 'relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Dense(512, activation = 'relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(1, activation = None))\n",
    "    model.summary()\n",
    "    # Configure model optimization\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'mse', metrics = [metrics.MeanAbsoluteError(), metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tRRLbSHgtRcz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1705730088295,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "tRRLbSHgtRcz",
    "outputId": "1ec4192a-c9ae-48e0-b269-241f8adbce23"
   },
   "outputs": [],
   "source": [
    "build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ybtF0BJgLBQ7",
   "metadata": {
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1705730146116,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "ybtF0BJgLBQ7"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_images, train_labels, test_images, test_labels, show_performance_by_epoch=False):\n",
    "    # Run model and get metrics for each epoch\n",
    "    performance_log = model.fit(\n",
    "        train_images, train_labels, epochs = 100,\n",
    "        batch_size = 64,\n",
    "        validation_data = (test_images, test_labels),\n",
    "        callbacks = [EarlyStopping(monitor = 'val_mean_absolute_error', patience = 5, restore_best_weights = True)]\n",
    "    )\n",
    "\n",
    "    if show_performance_by_epoch:\n",
    "        performance_by_epoch(performance_log)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rklG2gT4MshS",
   "metadata": {
    "executionInfo": {
     "elapsed": 730,
     "status": "ok",
     "timestamp": 1705730156903,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "rklG2gT4MshS"
   },
   "outputs": [],
   "source": [
    "def performance_by_epoch(performance_log):\n",
    "    # Get metrics for each epoch after model finishes training\n",
    "    train_loss = performance_log.history['loss']\n",
    "    test_loss = performance_log.history['val_loss']\n",
    "    train_mae = performance_log.history['mean_absolute_error']\n",
    "    test_mae = performance_log.history['val_mean_absolute_error']\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    # Build a dataframe storing epoch metrics\n",
    "    performance_df = pd.DataFrame(columns = ['epoch', 'train_or_test', 'loss_or_mae', 'value'])\n",
    "    for i in range(len(train_loss)):\n",
    "        new_row = {'epoch': epochs[i], 'train_or_test': 'train', 'loss_or_mae': 'loss', 'value': train_loss[i]}\n",
    "        performance_df = performance_df.append(new_row, ignore_index=True)\n",
    "        new_row = {'epoch': epochs[i], 'train_or_test': 'test', 'loss_or_mae': 'loss', 'value': test_loss[i]}\n",
    "        performance_df = performance_df.append(new_row, ignore_index=True)\n",
    "        new_row = {'epoch': epochs[i], 'train_or_test': 'train', 'loss_or_mae': 'mae', 'value': train_mae[i]}\n",
    "        performance_df = performance_df.append(new_row, ignore_index=True)\n",
    "        new_row = {'epoch': epochs[i], 'train_or_test': 'test', 'loss_or_mae': 'mae', 'value': test_mae[i]}\n",
    "        performance_df = performance_df.append(new_row, ignore_index=True)\n",
    "    performance_df = performance_df.astype({'epoch': np.int64})\n",
    "\n",
    "    # Plot metrics on graph, fitted with exponential decay curves\n",
    "    lm = sns.lmplot(x = 'epoch', y = 'value', data = performance_df, row = 'loss_or_mae', logx = True, truncate = False, sharey = False, hue = 'train_or_test')  # Note: If epoch = 1, this line causes an error. Make sure epoch >= 2\n",
    "    axes = lm.axes\n",
    "    max_mae = performance_df.loc[performance_df.loss_or_mae == 'mae']['value'].max()\n",
    "    min_mae = performance_df.loc[performance_df.loss_or_mae == 'mae']['value'].min()\n",
    "    axes[1, 0].set_ylim(min_mae - min_mae * 0.2, max_mae + max_mae * 0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g0O9QrM6Ldw8",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1705730160346,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "g0O9QrM6Ldw8"
   },
   "outputs": [],
   "source": [
    "def generate_predictions(model, test_images, test_labels):\n",
    "    # Run validation data through model and print mean absolute error\n",
    "    raw_predictions = model.predict(test_images)\n",
    "    raw_predictions = raw_predictions.flatten()\n",
    "\n",
    "    # Build a dataframe storing data for each prediction made by the model\n",
    "    processed_predictions = pd.DataFrame(columns = ['prediction', 'actual', 'abs_error', 'category'])\n",
    "    for i in range(len(raw_predictions)):\n",
    "        abs_error = abs(raw_predictions[i] - test_labels[i])\n",
    "        new_row = {'prediction': raw_predictions[i], 'actual': test_labels[i], 'abs_error': abs_error, 'abs_error_squared': abs_error ** 2, 'category': category_of(test_labels[i])}\n",
    "        processed_predictions = processed_predictions.append(new_row, ignore_index = True)\n",
    "        print_progress('Processing Predictions', i + 1, len(raw_predictions))\n",
    "\n",
    "    return processed_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VwUQ2WvcM0d8",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1705730178442,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "VwUQ2WvcM0d8"
   },
   "outputs": [],
   "source": [
    "def show_validation_results(predictions, show_plots = True, print_error = True):\n",
    "    print('\\n\\nRESULTS')\n",
    "\n",
    "    if print_error:\n",
    "        mae = predictions['abs_error'].mean()\n",
    "        print('\\nMean Absolute Error: ' + str(round(float(mae), 2)) + ' knots')\n",
    "        rmse = predictions['abs_error_squared'].mean() ** 0.5\n",
    "        print('Root Mean Square Error: ' + str(round(float(rmse), 2)) + ' knots')\n",
    "\n",
    "    if show_plots:\n",
    "        # List of categories in order of ascending strength\n",
    "        categories = ['T. Depression', 'T. Storm', 'Category 1', 'Category 2', 'Category 3', 'Category 4', 'Category 5']\n",
    "\n",
    "        # Show bar graph of median absolute error for each category\n",
    "        plt.figure(figsize = (8, 8), dpi = 300)\n",
    "        sns.barplot(x = 'category', y = 'abs_error', data = predictions, estimator = np.median, order = categories)\n",
    "        sns.despine()\n",
    "        plt.tight_layout()\n",
    "        plt.xlabel(\"Hurricane Strength\")\n",
    "        plt.ylabel(\"Absolute Error\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Median Absolute Error in Neural Network's Predictions By Category\")\n",
    "        plt.plot()\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "        # Show density plot of error for each category\n",
    "        for category in categories:\n",
    "            num_samples_tested = len(predictions.loc[predictions.category == category]['abs_error'])\n",
    "            sns.distplot(predictions.loc[predictions.category == category]['abs_error'], label = category + ' (' + str(num_samples_tested) + ' samples tested)', hist = False, kde_kws = {\"shade\": True})\n",
    "            sns.despine()\n",
    "        plt.xlabel(\"Absolute Error\")\n",
    "        plt.title(\"Distribution of Absolute Error By Category\")\n",
    "        plt.legend()\n",
    "        plt.xlim(0, None)\n",
    "        plt.ylim(0, None)\n",
    "        print('Graph of error distribution by category saved as error_dist_by_category.png')\n",
    "        plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z1Y2wFYmLncf",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1705730178443,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "z1Y2wFYmLncf"
   },
   "outputs": [],
   "source": [
    "def standardize_data(train_images, test_images):\n",
    "    train_images[train_images < 0] = 0\n",
    "    test_images[test_images < 0] = 0\n",
    "    st_dev = np.std(train_images)\n",
    "    mean = np.mean(train_images)\n",
    "    train_images = np.divide(np.subtract(train_images, mean), st_dev)\n",
    "    test_images = np.divide(np.subtract(test_images, mean), st_dev)\n",
    "    return train_images, test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uWHDNJtkKZut",
   "metadata": {
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1705732458951,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "uWHDNJtkKZut"
   },
   "outputs": [],
   "source": [
    "def category_of(wind_speed):\n",
    "    if wind_speed <= 33:\n",
    "        return 'T. Depression'\n",
    "    elif wind_speed <= 64:\n",
    "        return 'T. Storm'\n",
    "    elif wind_speed <= 83:\n",
    "        return 'Category 1'\n",
    "    elif wind_speed <= 95:\n",
    "        return 'Category 2'\n",
    "    elif wind_speed <= 113:\n",
    "        return 'Category 3'\n",
    "    elif wind_speed <= 134:\n",
    "        return 'Category 4'\n",
    "    else:\n",
    "        return 'Category 5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Va4Qc2smrQTW",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1705730191519,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "Va4Qc2smrQTW"
   },
   "outputs": [],
   "source": [
    "def download_hursat(years, path):\n",
    "    # Reads in the Best Track dataset, which contain records of the location and maximum wind speed of every recorded hurricane in the Atlantic and Eastern/Central Pacific basins\n",
    "    best_track_data = pd.read_csv(path)\n",
    "\n",
    "    for year in years:\n",
    "        # Scrapes a webpage to get list of all .tar.gz files. Each file contains all the satellite images associated with a particular hurricane.\n",
    "        year_directory_url = 'https://www.ncei.noaa.gov/data/hurricane-satellite-hursat-b1/archive/v06/' + year\n",
    "        year_directory_page = requests.get(year_directory_url).text\n",
    "        year_directory_soup = BeautifulSoup(year_directory_page, 'html.parser')\n",
    "        year_directory_file_urls = [year_directory_url + '/' + node.get('href') for node in year_directory_soup.find_all('a') if node.get('href').endswith('tar.gz')]\n",
    "        print('\\n' + year + ' file loaded.')\n",
    "\n",
    "        files_processed = 0\n",
    "        for storm_file_url in year_directory_file_urls:\n",
    "            # Determine whether the best track dataset has information about this particular hurricane. This filters\n",
    "            # out storms in basins other than the Atlantic or Pacific, since the best track dataset doesn't have information for those storms.\n",
    "            storm_name = storm_file_url.split('_')[-2]\n",
    "            year = int(storm_file_url.split('_')[3][:4])\n",
    "            print(year)\n",
    "            file_has_match_in_best_track = not best_track_data.loc[(best_track_data['year'] == year) & (best_track_data['storm_name'] == storm_name)].empty\n",
    "\n",
    "            if file_has_match_in_best_track:\n",
    "                # Build a string, which will be file path where we save the .tar.gz when downloaded\n",
    "                file_name = storm_file_url.split('/')[-1]\n",
    "                storm_file_path = 'Satellite Imagery/' + file_name\n",
    "\n",
    "                # Create the Satellite Imagery folder if it doesn't already exist\n",
    "                if not os.path.exists('Satellite Imagery'):\n",
    "                    os.makedirs('Satellite Imagery')\n",
    "\n",
    "                # Open the .tar.gz and copy it's contents from the web, onto our computer\n",
    "                request = requests.get(storm_file_url, allow_redirects = True)\n",
    "                open(storm_file_path, 'wb').write(request.content)\n",
    "                request.close()\n",
    "\n",
    "                # Open the .tar.gz file and loop through each file inside. Each of these netcdf files contains a satellite image of a hurricane at a moment in time\n",
    "                tar = tarfile.open(storm_file_path)\n",
    "                file_prefixes_in_directory = []\n",
    "                for file_name in tar.getnames():\n",
    "                    # Get the date and time of the satellite image, and the name of the satellite that took the image\n",
    "                    fulldate = file_name.split(\".\")[2] + file_name.split(\".\")[3] + file_name.split(\".\")[4]\n",
    "                    time = file_name.split(\".\")[5]\n",
    "                    satellite = file_name.split(\".\")[7][:3]\n",
    "\n",
    "                    # Determine whether the best track dataset has a record for the date and time of this storm.\n",
    "                    file_has_match_in_best_track = not best_track_data.loc[(best_track_data['fulldate'] == int(fulldate)) & (best_track_data['time'] == int(time))].empty\n",
    "\n",
    "                    # Determine whether another image of this hurricane at this exact time has already been extracted from the .tar.gz\n",
    "                    is_redundant = '.'.join(file_name.split('.')[:6]) in file_prefixes_in_directory\n",
    "\n",
    "                    # If the requirements are met, extract the netcdf file from this .tar.gz and save it locally\n",
    "                    if file_has_match_in_best_track and not is_redundant and satellite == \"GOE\":\n",
    "                        f = tar.extractfile(file_name)\n",
    "                        open('Satellite Imagery/' + file_name, 'wb').write(f.read())\n",
    "                        file_prefixes_in_directory.append('.'.join(file_name.split('.')[:6]))\n",
    "\n",
    "                tar.close()\n",
    "                os.remove(storm_file_path)\n",
    "\n",
    "            files_processed += 1\n",
    "            print_progress('Processing Files for ' + str(year), files_processed, len(year_directory_file_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ipiqLksXE5-H",
   "metadata": {
    "id": "ipiqLksXE5-H"
   },
   "source": [
    "# **Run only once**\n",
    "If you already have the generated files saved in your google drive, do not run the below code again, because there is a limit on google colab time for running the codebase, your code might stop after the threshold time limit of 90 minutes is reached, to safeguard that process, run the below codes for each basin only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zvDHcjUadEf3",
   "metadata": {
    "id": "zvDHcjUadEf3"
   },
   "outputs": [],
   "source": [
    "# replace the file path for each basin, the below code is an example of how the paths have to be changed\n",
    "# csv_path = '/content/drive/MyDrive/HURSAT/hursat/ibtracs_NA.csv'\n",
    "# image_path = '/content/drive/MyDrive/HURSAT/hursat/NA_images.npy'\n",
    "# label_path = '/content/drive/MyDrive/HURSAT/hursat/NA_labels.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DocJdl55uLyz",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1705730435786,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "DocJdl55uLyz"
   },
   "outputs": [],
   "source": [
    "csv_path = '/content/drive/MyDrive/HURSAT/hursat/ibtracs_NA.csv'\n",
    "image_path = '/content/drive/MyDrive/HURSAT/hursat/NA_images.npy'\n",
    "label_path = '/content/drive/MyDrive/HURSAT/hursat/NA_labels.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CTd8Nnr61QYM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CTd8Nnr61QYM",
    "outputId": "3a0ccdc0-fe20-4c6c-e478-8ac7632f4ede"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Specify a list of years. Satellite images of hurricanes from those years will be downloaded.\n",
    "    # More years will provide more data for the neural network to work with in model.py, but will take longer to download.\n",
    "    YEARS_TO_DOWNLOAD = ['2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016']\n",
    "    download_hursat(YEARS_TO_DOWNLOAD, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b32aa61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 266078,
     "status": "ok",
     "timestamp": 1705731710282,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "7b32aa61",
    "outputId": "87e6cba8-81b5-4f69-da22-a167c3eb0985"
   },
   "outputs": [],
   "source": [
    "# Reads in the Best Track dataset, which contain records of the location and maximum wind speed of every recorded hurricane in the Atlantic and Eastern/Central Pacific basins\n",
    "best_track_data = pd.read_csv(csv_path)\n",
    "\n",
    "# The number of pixels wide and tall to crop the images of hurricanes to\n",
    "side_length = 50\n",
    "\n",
    "# Lists to hold the hurricane images and the wind speed associated with those images. These lists are aligned so that\n",
    "# the first image in the images list corresponds to the first label in the labels list.\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Gets list of names of files, each file containing a satellite image\n",
    "files = os.listdir('Satellite Imagery')\n",
    "num_files = len(files)\n",
    "\n",
    "for i in range(len(files)):\n",
    "    # Get IR satellite image from the file\n",
    "    raw_data = netCDF4.Dataset('Satellite Imagery/' + files[i])\n",
    "    ir_data = raw_data.variables['IRWIN'][0]\n",
    "\n",
    "    # 'Crop' the image by removing north, south, east, and west edges\n",
    "    south_bound = (ir_data.shape[0] - side_length) // 2\n",
    "    north_bound = south_bound + side_length\n",
    "    cropped_ir_data = ir_data[south_bound:north_bound]\n",
    "    west_bound = (ir_data.shape[1] - side_length) // 2\n",
    "    east_bound = side_length\n",
    "    cropped_ir_data = np.delete(cropped_ir_data, np.s_[:west_bound], axis=1)\n",
    "    cropped_ir_data = np.delete(cropped_ir_data, np.s_[east_bound:], axis=1)\n",
    "\n",
    "    # Get storm name, date, and time of the hurricane from the image's file name\n",
    "    file_name = files[i]\n",
    "    file_name = file_name.split('.')\n",
    "    storm_name = file_name[1]\n",
    "    date = int(file_name[2] + file_name[3] + file_name[4])\n",
    "    time = int(file_name[5])\n",
    "\n",
    "    # Filter the best track dataset to find the row that matches the name, date, and time of this hurricane image\n",
    "    matching_best_track_data = best_track_data.loc[(best_track_data.storm_name == storm_name) & (best_track_data.fulldate == date) & (best_track_data.time == time)]\n",
    "\n",
    "    # Get the wind speed from the row that matches the name, date, and time of this hurricane image\n",
    "    try:\n",
    "        wind_speed = matching_best_track_data.max_sus_wind_speed.reset_index(drop = True)[0]\n",
    "    except Exception:\n",
    "        print('\\rCould not find label for image of ' + storm_name + ' at date ' + str(date) + ' and time ' + str(time), end='\\n')\n",
    "        continue  # Skip to the next hurricane image if the a wind speed could not be found for this hurricane image\n",
    "\n",
    "    # Add the image and wind speed to these lists. This way, the lists of images and labels always line up. The first hurricane image in the images list is associated with the first wind speed in the labels list.\n",
    "    images.append(cropped_ir_data)\n",
    "    labels.append(wind_speed)\n",
    "\n",
    "    raw_data.close()\n",
    "\n",
    "    print('\\rProcessing Samples... ' + str(round(((i + 1) / num_files) * 100, 1)) + '% (' + str(i + 1) + ' of ' + str(num_files) + ')', end='')\n",
    "\n",
    "print('\\nSaving NumPy arrays...')\n",
    "\n",
    "# Turn the list of images and labels into NumPy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Add a fourth dimension to the images array. This is one since we only have one color channel: grayscale. The fourth dimension would typically be 3 if we were working with color images\n",
    "images = images.reshape((images.shape[0], side_length, side_length, 1))\n",
    "\n",
    "# Save the NumPy arrays for use in model.py, where the neural network is trained and validated on this data\n",
    "np.save(image_path, images)\n",
    "np.save(label_path, labels)\n",
    "\n",
    "print(\"\\nNumPy files saved. Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672864aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1992,
     "status": "ok",
     "timestamp": 1705731774597,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "672864aa",
    "outputId": "fb177774-d6bc-436a-a0c3-c205f9962338"
   },
   "outputs": [],
   "source": [
    "images = np.load(image_path)\n",
    "labels = np.load(label_path)\n",
    "\n",
    "print('Number of images:', len(images), '\\n')\n",
    "print('Number of labels:', len(labels), '\\n')\n",
    "\n",
    "for x in range(5):\n",
    "    i = random.randint(0, images.shape[0])\n",
    "    image = np.reshape(images[i], (images[i].shape[0], images[i].shape[1]))\n",
    "    plt.imshow(image, cmap = 'binary')\n",
    "    plt.title('Image #' + str(i) + '   ' + str(labels[i]) + ' knots')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oFflX-w7m4C0",
   "metadata": {
    "id": "oFflX-w7m4C0"
   },
   "source": [
    "# **North Atlantic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bM2gm3dt3Vr",
   "metadata": {
    "executionInfo": {
     "elapsed": 453,
     "status": "ok",
     "timestamp": 1705732609231,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "6bM2gm3dt3Vr"
   },
   "outputs": [],
   "source": [
    "csv_path = '/content/drive/MyDrive/HURSAT/hursat/ibtracs_NA.csv'\n",
    "image_path = '/content/drive/MyDrive/HURSAT/hursat/NA_images.npy'\n",
    "label_path = '/content/drive/MyDrive/HURSAT/hursat/NA_labels.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hq9y5pMJWmcJ",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1705732609866,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "hq9y5pMJWmcJ"
   },
   "outputs": [],
   "source": [
    "data = np.load(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_XB2eiamim_9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1705732609867,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "_XB2eiamim_9",
    "outputId": "39498ee2-9a07-4056-af63-022e83188653"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns = ['Wind Speed'])\n",
    "display(df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70JvGCJQig2X",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 619,
     "status": "ok",
     "timestamp": 1705732688398,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "70JvGCJQig2X",
    "outputId": "17394cb8-2e9b-486c-9574-2d0022a0a068"
   },
   "outputs": [],
   "source": [
    "df['Wind Speed'] = df['Wind Speed'].apply(lambda row: category_of(row))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UjVoU_NzkodH",
   "metadata": {
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1705732693872,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "UjVoU_NzkodH"
   },
   "outputs": [],
   "source": [
    "df.to_csv('/content/drive/MyDrive/HURSAT/hursat/NA_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qDD24pe4NvYI",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1705732695319,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "qDD24pe4NvYI"
   },
   "outputs": [],
   "source": [
    "def read_and_prepare_data(validation_mode, k = 5, augment = True):\n",
    "    if validation_mode == 'k_fold':\n",
    "\n",
    "        # Read in data from files\n",
    "        images = np.load(image_path)\n",
    "        labels = np.load(label_path)\n",
    "\n",
    "        # Split the image and label datasets into k number of subsets\n",
    "        folded_images = []\n",
    "        folded_labels = []\n",
    "        for i in range(k):\n",
    "            start = int((i / k) * len(images))\n",
    "            end = int(((i + 1) / k) * len(images))\n",
    "            folded_images.append(images[start:end])\n",
    "            folded_labels.append(labels[start:end])\n",
    "\n",
    "        # Generate augmented images for each fold\n",
    "        folded_augmented_images = []\n",
    "        folded_augmented_labels = []\n",
    "        for i in range(k):\n",
    "            if augment:\n",
    "                print('\\nAugmenting Fold ' + str(i + 1) + ' of ' + str(k))\n",
    "                augmented_images, augmented_labels = augment_images(folded_images[i], folded_labels[i])\n",
    "                folded_augmented_images.append(augmented_images)\n",
    "                folded_augmented_labels.append(augmented_labels)\n",
    "\n",
    "        # Combine the folds into sets for each iteration of the model and standardize the data\n",
    "        train_images = []\n",
    "        train_labels = []\n",
    "        test_images = []\n",
    "        test_labels = []\n",
    "        for i in range(k):\n",
    "            train_images.append(np.concatenate(folded_images[:i] + folded_images[(i+1):]))\n",
    "            train_labels.append(np.concatenate(folded_labels[:i] + folded_labels[(i+1):]))\n",
    "            if augment:\n",
    "                train_images[i] = np.concatenate(([train_images[i]] + folded_augmented_images[:i] + folded_augmented_images[(i + 1):]))\n",
    "                train_labels[i] = np.concatenate(([train_labels[i]] + folded_augmented_labels[:i] + folded_augmented_labels[(i + 1):]))\n",
    "            test_images.append(folded_images[i])\n",
    "            test_labels.append(folded_labels[i])\n",
    "            train_images[i], test_images[i] = standardize_data(train_images[i], test_images[i])\n",
    "\n",
    "        return train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Or9UzER75xr-",
   "metadata": {
    "id": "Or9UzER75xr-"
   },
   "source": [
    "This file takes the satellite image files downloaded and turns them into NumPy arrays that will be fed to the neural network\n",
    "\n",
    "Outline of this file:\n",
    "- Crops each satellite image\n",
    "- Matches each satellite image of a hurricane with the maximum sustained wind speed of that hurricane\n",
    "- Collects these images and their associated labels in arrays and saves them as numpy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0daea77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0daea77",
    "outputId": "f3723ed6-b13c-477f-c71c-65177bdbbfc9"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Specify whether the script should use Keras's ImageDataGenerator to augment the training dataset. Assigning\n",
    "    # this variable to True will improve accuracy, but will also increase execution time.\n",
    "    AUGMENT = True\n",
    "\n",
    "    # Specify how many folds in the k-fold validation process. Can be any integer greater than or equal to 2. Larger\n",
    "    # integers will increase execution time.\n",
    "    NUM_FOLDS = 5\n",
    "\n",
    "    train_images, train_labels, test_images, test_labels = read_and_prepare_data('k_fold', NUM_FOLDS, augment = AUGMENT)\n",
    "    model = build_model()\n",
    "    predictions = pd.DataFrame(columns = ['prediction', 'actual', 'abs_error', 'category'])\n",
    "    for i in range(NUM_FOLDS):\n",
    "        print('\\n\\nTraining Fold ' + str(i + 1) + ' of ' + str(NUM_FOLDS) + '\\n')\n",
    "        model = train_model(model, train_images[i], train_labels[i], test_images[i], test_labels[i])\n",
    "        kth_fold_predictions = generate_predictions(model, test_images[i], test_labels[i])\n",
    "        predictions = predictions.append(kth_fold_predictions, ignore_index = True)\n",
    "    show_validation_results(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yV8RUlUzCY_I",
   "metadata": {
    "id": "yV8RUlUzCY_I"
   },
   "outputs": [],
   "source": [
    "model.save('/content/drive/MyDrive/hursat/NA.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M5WlEGKkXSLZ",
   "metadata": {
    "id": "M5WlEGKkXSLZ"
   },
   "source": [
    "# **North Pacific**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UdZqv7DE3-Wl",
   "metadata": {
    "id": "UdZqv7DE3-Wl"
   },
   "outputs": [],
   "source": [
    "csv_path = '/content/drive/MyDrive/HURSAT/hursat/ibtracs_NP.csv'\n",
    "image_path = '/content/drive/MyDrive/HURSAT/hursat/NP_images.npy'\n",
    "label_path = '/content/drive/MyDrive/HURSAT/hursat/NP_labels.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P3W4M6T5ClK8",
   "metadata": {
    "id": "P3W4M6T5ClK8"
   },
   "outputs": [],
   "source": [
    "data = np.load(label_path)\n",
    "df = pd.DataFrame(data, columns = ['Wind Speed'])\n",
    "df['Wind Speed'] = df.apply(lambda row: categorise(row), axis = 1)\n",
    "df.to_csv('/content/drive/MyDrive/hursat/NP_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mBWx4lwcCOvF",
   "metadata": {
    "id": "mBWx4lwcCOvF"
   },
   "outputs": [],
   "source": [
    "def read_and_prepare_data(validation_mode, k = 5, augment = True):\n",
    "    if validation_mode == 'k_fold':\n",
    "\n",
    "        # Read in data from files\n",
    "        images = np.load(image_path)\n",
    "        labels = np.load(label_path)\n",
    "\n",
    "        # Split the image and label datasets into k number of subsets\n",
    "        folded_images = []\n",
    "        folded_labels = []\n",
    "        for i in range(k):\n",
    "            start = int((i / k) * len(images))\n",
    "            end = int(((i + 1) / k) * len(images))\n",
    "            folded_images.append(images[start:end])\n",
    "            folded_labels.append(labels[start:end])\n",
    "\n",
    "        # Generate augmented images for each fold\n",
    "        folded_augmented_images = []\n",
    "        folded_augmented_labels = []\n",
    "        for i in range(k):\n",
    "            if augment:\n",
    "                print('\\nAugmenting Fold ' + str(i + 1) + ' of ' + str(k))\n",
    "                augmented_images, augmented_labels = augment_images(folded_images[i], folded_labels[i])\n",
    "                folded_augmented_images.append(augmented_images)\n",
    "                folded_augmented_labels.append(augmented_labels)\n",
    "\n",
    "        # Combine the folds into sets for each iteration of the model and standardize the data\n",
    "        train_images = []\n",
    "        train_labels = []\n",
    "        test_images = []\n",
    "        test_labels = []\n",
    "        for i in range(k):\n",
    "            train_images.append(np.concatenate(folded_images[:i] + folded_images[(i+1):]))\n",
    "            train_labels.append(np.concatenate(folded_labels[:i] + folded_labels[(i+1):]))\n",
    "            if augment:\n",
    "                train_images[i] = np.concatenate(([train_images[i]] + folded_augmented_images[:i] + folded_augmented_images[(i + 1):]))\n",
    "                train_labels[i] = np.concatenate(([train_labels[i]] + folded_augmented_labels[:i] + folded_augmented_labels[(i + 1):]))\n",
    "            test_images.append(folded_images[i])\n",
    "            test_labels.append(folded_labels[i])\n",
    "            train_images[i], test_images[i] = standardize_data(train_images[i], test_images[i])\n",
    "\n",
    "        return train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3CKBAT3Lce31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2876606,
     "status": "ok",
     "timestamp": 1673641035809,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "3CKBAT3Lce31",
    "outputId": "7f511c91-9c79-4d37-d310-da9a70b932a5"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    AUGMENT = True\n",
    "\n",
    "    NUM_FOLDS = 5\n",
    "\n",
    "    train_images, train_labels, test_images, test_labels = read_and_prepare_data('k_fold', NUM_FOLDS, augment = AUGMENT)\n",
    "    model = build_model()\n",
    "    predictions = pd.DataFrame(columns = ['prediction', 'actual', 'abs_error', 'category'])\n",
    "    for i in range(NUM_FOLDS):\n",
    "        print('\\n\\nTraining Fold ' + str(i + 1) + ' of ' + str(NUM_FOLDS) + '\\n')\n",
    "        model = train_model(model, train_images[i], train_labels[i], test_images[i], test_labels[i])\n",
    "        kth_fold_predictions = generate_predictions(model, test_images[i], test_labels[i])\n",
    "        predictions = predictions.append(kth_fold_predictions, ignore_index = True)\n",
    "    show_validation_results(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OB4I94kgqdeH",
   "metadata": {
    "id": "OB4I94kgqdeH"
   },
   "outputs": [],
   "source": [
    "model.save('/content/drive/MyDrive/hursat/NP.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sxTPnrcHTHjH",
   "metadata": {
    "id": "sxTPnrcHTHjH"
   },
   "source": [
    "# **North Indian**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E0mUSQ4XxVuE",
   "metadata": {
    "id": "E0mUSQ4XxVuE"
   },
   "outputs": [],
   "source": [
    "csv_path = '/content/drive/MyDrive/HURSAT/hursat/ibtracs_NI.csv'\n",
    "image_path = '/content/drive/MyDrive/HURSAT/hursat/NI_images.npy'\n",
    "label_path = '/content/drive/MyDrive/HURSAT/hursat/NI_labels.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CXWz_BTiCGCe",
   "metadata": {
    "id": "CXWz_BTiCGCe"
   },
   "outputs": [],
   "source": [
    "def read_and_prepare_data(validation_mode, k = 5, augment = True):\n",
    "    if validation_mode == 'k_fold':\n",
    "\n",
    "        # Read in data from files\n",
    "        images = np.load(image_path)\n",
    "        labels = np.load(label_path)\n",
    "\n",
    "        # Split the image and label datasets into k number of subsets\n",
    "        folded_images = []\n",
    "        folded_labels = []\n",
    "        for i in range(k):\n",
    "            start = int((i / k) * len(images))\n",
    "            end = int(((i + 1) / k) * len(images))\n",
    "            folded_images.append(images[start:end])\n",
    "            folded_labels.append(labels[start:end])\n",
    "\n",
    "        # Generate augmented images for each fold\n",
    "        folded_augmented_images = []\n",
    "        folded_augmented_labels = []\n",
    "        for i in range(k):\n",
    "            if augment:\n",
    "                print('\\nAugmenting Fold ' + str(i + 1) + ' of ' + str(k))\n",
    "                augmented_images, augmented_labels = augment_images(folded_images[i], folded_labels[i])\n",
    "                folded_augmented_images.append(augmented_images)\n",
    "                folded_augmented_labels.append(augmented_labels)\n",
    "\n",
    "        # Combine the folds into sets for each iteration of the model and standardize the data\n",
    "        train_images = []\n",
    "        train_labels = []\n",
    "        test_images = []\n",
    "        test_labels = []\n",
    "        for i in range(k):\n",
    "            train_images.append(np.concatenate(folded_images[:i] + folded_images[(i+1):]))\n",
    "            train_labels.append(np.concatenate(folded_labels[:i] + folded_labels[(i+1):]))\n",
    "            if augment:\n",
    "                train_images[i] = np.concatenate(([train_images[i]] + folded_augmented_images[:i] + folded_augmented_images[(i + 1):]))\n",
    "                train_labels[i] = np.concatenate(([train_labels[i]] + folded_augmented_labels[:i] + folded_augmented_labels[(i + 1):]))\n",
    "            test_images.append(folded_images[i])\n",
    "            test_labels.append(folded_labels[i])\n",
    "            train_images[i], test_images[i] = standardize_data(train_images[i], test_images[i])\n",
    "\n",
    "        return train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q0BC8ZvQUeJY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 74666,
     "status": "ok",
     "timestamp": 1673641166246,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "Q0BC8ZvQUeJY",
    "outputId": "3c8815ed-e1d7-4076-d2dd-92b091721ebe"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    AUGMENT = True\n",
    "    NUM_FOLDS = 5\n",
    "\n",
    "    train_images, train_labels, test_images, test_labels = read_and_prepare_data('k_fold', NUM_FOLDS, augment = AUGMENT)\n",
    "    model = build_model()\n",
    "    predictions = pd.DataFrame(columns = ['prediction', 'actual', 'abs_error', 'category'])\n",
    "    for i in range(NUM_FOLDS):\n",
    "        print('\\n\\nTraining Fold ' + str(i + 1) + ' of ' + str(NUM_FOLDS) + '\\n')\n",
    "        model = train_model(model, train_images[i], train_labels[i], test_images[i], test_labels[i])\n",
    "        kth_fold_predictions = generate_predictions(model, test_images[i], test_labels[i])\n",
    "        predictions = predictions.append(kth_fold_predictions, ignore_index = True)\n",
    "    show_validation_results(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bB9ecMeB3nnp",
   "metadata": {
    "id": "bB9ecMeB3nnp"
   },
   "outputs": [],
   "source": [
    "model.save('/content/drive/MyDrive/hursat/NI.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CH9dn4YpEbLl",
   "metadata": {
    "id": "CH9dn4YpEbLl"
   },
   "source": [
    "# **North Atlantic + North Pacific + North Indian**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rX-WQbkjU7JR",
   "metadata": {
    "id": "rX-WQbkjU7JR"
   },
   "outputs": [],
   "source": [
    "csv_path = '/content/drive/MyDrive/hursat/ibtracs_NP_NI_NA.csv'\n",
    "image_path = '/content/drive/MyDrive/hursat/NP_NI_NA_images.npy'\n",
    "label_path = '/content/drive/MyDrive/hursat/NP_NI_NA_labels.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fn_XBE8otGP2",
   "metadata": {
    "id": "Fn_XBE8otGP2"
   },
   "outputs": [],
   "source": [
    "def read_and_prepare_data(validation_mode, k = 5, augment = True):\n",
    "    if validation_mode == 'k_fold':\n",
    "\n",
    "        # Read in data from files\n",
    "        images = np.load(image_path)\n",
    "        labels = np.load(label_path)\n",
    "\n",
    "        # Split the image and label datasets into k number of subsets\n",
    "        folded_images = []\n",
    "        folded_labels = []\n",
    "        for i in range(k):\n",
    "            start = int((i / k) * len(images))\n",
    "            end = int(((i + 1) / k) * len(images))\n",
    "            folded_images.append(images[start:end])\n",
    "            folded_labels.append(labels[start:end])\n",
    "\n",
    "        # Generate augmented images for each fold\n",
    "        folded_augmented_images = []\n",
    "        folded_augmented_labels = []\n",
    "        for i in range(k):\n",
    "            if augment:\n",
    "                print('\\nAugmenting Fold ' + str(i + 1) + ' of ' + str(k))\n",
    "                augmented_images, augmented_labels = augment_images(folded_images[i], folded_labels[i])\n",
    "                folded_augmented_images.append(augmented_images)\n",
    "                folded_augmented_labels.append(augmented_labels)\n",
    "\n",
    "        # Combine the folds into sets for each iteration of the model and standardize the data\n",
    "        train_images = []\n",
    "        train_labels = []\n",
    "        test_images = []\n",
    "        test_labels = []\n",
    "        for i in range(k):\n",
    "            train_images.append(np.concatenate(folded_images[:i] + folded_images[(i+1):]))\n",
    "            train_labels.append(np.concatenate(folded_labels[:i] + folded_labels[(i+1):]))\n",
    "            if augment:\n",
    "                train_images[i] = np.concatenate(([train_images[i]] + folded_augmented_images[:i] + folded_augmented_images[(i + 1):]))\n",
    "                train_labels[i] = np.concatenate(([train_labels[i]] + folded_augmented_labels[:i] + folded_augmented_labels[(i + 1):]))\n",
    "            test_images.append(folded_images[i])\n",
    "            test_labels.append(folded_labels[i])\n",
    "            train_images[i], test_images[i] = standardize_data(train_images[i], test_images[i])\n",
    "\n",
    "        return train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pJo4qBFGEul4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2494306,
     "status": "ok",
     "timestamp": 1665931058835,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "pJo4qBFGEul4",
    "outputId": "ad5f646d-4ba8-4393-e57d-825782e2aa1b"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    AUGMENT = True\n",
    "    NUM_FOLDS = 5\n",
    "\n",
    "    train_images, train_labels, test_images, test_labels = read_and_prepare_data('k_fold', NUM_FOLDS, augment = AUGMENT)\n",
    "    model = build_model()\n",
    "    predictions = pd.DataFrame(columns=['prediction', 'actual', 'abs_error', 'category'])\n",
    "    for i in range(NUM_FOLDS):\n",
    "        print('\\n\\nTraining Fold ' + str(i + 1) + ' of ' + str(NUM_FOLDS) + '\\n')\n",
    "        model = train_model(model, train_images[i], train_labels[i], test_images[i], test_labels[i])\n",
    "        kth_fold_predictions = generate_predictions(model, test_images[i], test_labels[i])\n",
    "        predictions = predictions.append(kth_fold_predictions, ignore_index = True)\n",
    "    show_validation_results(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XO8Wfq1WdCu2",
   "metadata": {
    "id": "XO8Wfq1WdCu2"
   },
   "outputs": [],
   "source": [
    "model.save('/content/drive/MyDrive/hursat/NP_NI_NA.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wTSnjfCBZYE7",
   "metadata": {
    "id": "wTSnjfCBZYE7"
   },
   "source": [
    "# **North Atlantic + North Pacific**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gcKlcUO7PBKA",
   "metadata": {
    "id": "gcKlcUO7PBKA"
   },
   "outputs": [],
   "source": [
    "csv_path = '/content/drive/MyDrive/hursat/ibtracs_NP_NA.csv'\n",
    "image_path = '/content/drive/MyDrive/hursat/NP_NA_images.npy'\n",
    "label_path = '/content/drive/MyDrive/hursat/NP_NA_labels.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73F2Xd8LLt3A",
   "metadata": {
    "id": "73F2Xd8LLt3A"
   },
   "outputs": [],
   "source": [
    "data = np.load(label_path)\n",
    "df = pd.DataFrame(data, columns = ['Wind Speed'])\n",
    "df['Wind Speed'] = df.apply(lambda row: categorise(row), axis = 1)\n",
    "df.to_csv('/content/drive/MyDrive/hursat/NP_NA_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C3qvN3PNPwp0",
   "metadata": {
    "id": "C3qvN3PNPwp0"
   },
   "outputs": [],
   "source": [
    "def read_and_prepare_data(validation_mode, k = 5, augment = True):\n",
    "    if validation_mode == 'k_fold':\n",
    "\n",
    "        # Read in data from files\n",
    "        images = np.load(image_path)\n",
    "        labels = np.load(label_path)\n",
    "\n",
    "        # Split the image and label datasets into k number of subsets\n",
    "        folded_images = []\n",
    "        folded_labels = []\n",
    "        for i in range(k):\n",
    "            start = int((i / k) * len(images))\n",
    "            end = int(((i + 1) / k) * len(images))\n",
    "            folded_images.append(images[start:end])\n",
    "            folded_labels.append(labels[start:end])\n",
    "\n",
    "        # Generate augmented images for each fold\n",
    "        folded_augmented_images = []\n",
    "        folded_augmented_labels = []\n",
    "        for i in range(k):\n",
    "            if augment:\n",
    "                print('\\nAugmenting Fold ' + str(i + 1) + ' of ' + str(k))\n",
    "                augmented_images, augmented_labels = augment_images(folded_images[i], folded_labels[i])\n",
    "                folded_augmented_images.append(augmented_images)\n",
    "                folded_augmented_labels.append(augmented_labels)\n",
    "\n",
    "        # Combine the folds into sets for each iteration of the model and standardize the data\n",
    "        train_images = []\n",
    "        train_labels = []\n",
    "        test_images = []\n",
    "        test_labels = []\n",
    "        for i in range(k):\n",
    "            train_images.append(np.concatenate(folded_images[:i] + folded_images[(i+1):]))\n",
    "            train_labels.append(np.concatenate(folded_labels[:i] + folded_labels[(i+1):]))\n",
    "            if augment:\n",
    "                train_images[i] = np.concatenate(([train_images[i]] + folded_augmented_images[:i] + folded_augmented_images[(i + 1):]))\n",
    "                train_labels[i] = np.concatenate(([train_labels[i]] + folded_augmented_labels[:i] + folded_augmented_labels[(i + 1):]))\n",
    "            test_images.append(folded_images[i])\n",
    "            test_labels.append(folded_labels[i])\n",
    "            train_images[i], test_images[i] = standardize_data(train_images[i], test_images[i])\n",
    "\n",
    "        return train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rqdVu0lBaI5F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5383663,
     "status": "ok",
     "timestamp": 1665153154192,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "rqdVu0lBaI5F",
    "outputId": "a1fe3477-a807-4ae6-8265-aa9e679afb3f"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    AUGMENT = True\n",
    "\n",
    "    NUM_FOLDS = 5\n",
    "\n",
    "    train_images, train_labels, test_images, test_labels = read_and_prepare_data('k_fold', NUM_FOLDS, augment=AUGMENT)\n",
    "    model = build_model()\n",
    "    predictions = pd.DataFrame(columns = ['prediction', 'actual', 'abs_error', 'category'])\n",
    "    for i in range(NUM_FOLDS):\n",
    "        print('\\n\\nTraining Fold ' + str(i + 1) + ' of ' + str(NUM_FOLDS) + '\\n')\n",
    "        model = train_model(model, train_images[i], train_labels[i], test_images[i], test_labels[i])\n",
    "        kth_fold_predictions = generate_predictions(model, test_images[i], test_labels[i])\n",
    "        predictions = predictions.append(kth_fold_predictions, ignore_index = True)\n",
    "    show_validation_results(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tZO9y0FNaapC",
   "metadata": {
    "id": "tZO9y0FNaapC"
   },
   "outputs": [],
   "source": [
    "model.save('/content/drive/MyDrive/hursat/NP_NA.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6z-GI-2TdtuN",
   "metadata": {
    "id": "6z-GI-2TdtuN"
   },
   "source": [
    "# **North Indian + North Atlantic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uvfmMcG1kOUa",
   "metadata": {
    "id": "uvfmMcG1kOUa"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Specify a list of years. Satellite images of hurricanes from those years will be downloaded.\n",
    "    YEARS_TO_DOWNLOAD = ['2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016']\n",
    "    path = '/content/drive/MyDrive/hursat/ibtracs_NI_NA.csv'\n",
    "    download_hursat(YEARS_TO_DOWNLOAD, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AQxhi4Dzldxb",
   "metadata": {
    "id": "AQxhi4Dzldxb"
   },
   "outputs": [],
   "source": [
    "# This file takes the satellite image files downloaded and turns them into NumPy arrays that will be fed to the neural network\n",
    "# Outline of this file:\n",
    "# - Crops each satellite image\n",
    "# - Matches each satellite image of a hurricane with the maximum sustained wind speed of that hurricane\n",
    "# - Collects these images and their associated labels in arrays and saves them as numpy files\n",
    "\n",
    "# Reads in the Best Track dataset, which contain records of the location and maximum wind speed of every recorded hurricane in the Atlantic and Eastern/Central Pacific basins\n",
    "best_track_data = pd.read_csv('/content/drive/MyDrive/hursat/ibtracs_NI_NA.csv')\n",
    "\n",
    "# The number of pixels wide and tall to crop the images of hurricanes to\n",
    "side_length = 50\n",
    "\n",
    "# Lists to hold the hurricane images and the wind speed associated with those images. These lists are aligned so that\n",
    "# the first image in the images list corresponds to the first label in the labels list.\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Gets list of names of files, each file containing a satellite image\n",
    "files = os.listdir('Satellite Imagery')\n",
    "num_files = len(files)\n",
    "\n",
    "for i in range(len(files)):\n",
    "    # Get IR satellite image from the file\n",
    "    raw_data = netCDF4.Dataset('Satellite Imagery/' + files[i])\n",
    "    ir_data = raw_data.variables['IRWIN'][0]\n",
    "\n",
    "    # 'Crop' the image by removing north, south, east, and west edges\n",
    "    south_bound = (ir_data.shape[0] - side_length) // 2\n",
    "    north_bound = south_bound + side_length\n",
    "    cropped_ir_data = ir_data[south_bound:north_bound]\n",
    "    west_bound = (ir_data.shape[1] - side_length) // 2\n",
    "    east_bound = side_length\n",
    "    cropped_ir_data = np.delete(cropped_ir_data, np.s_[:west_bound], axis=1)\n",
    "    cropped_ir_data = np.delete(cropped_ir_data, np.s_[east_bound:], axis=1)\n",
    "\n",
    "    # Get storm name, date, and time of the hurricane from the image's file name\n",
    "    file_name = files[i]\n",
    "    file_name = file_name.split('.')\n",
    "    storm_name = file_name[1]\n",
    "    date = int(file_name[2] + file_name[3] + file_name[4])\n",
    "    time = int(file_name[5])\n",
    "\n",
    "    # Filter the best track dataset to find the row that matches the name, date, and time of this hurricane image\n",
    "    matching_best_track_data = best_track_data.loc[(best_track_data.storm_name == storm_name) & (best_track_data.fulldate == date) & (best_track_data.time == time)]\n",
    "\n",
    "    # Get the wind speed from the row that matches the name, date, and time of this hurricane image\n",
    "    try:\n",
    "        wind_speed = matching_best_track_data.max_sus_wind_speed.reset_index(drop = True)[0]\n",
    "    except Exception:\n",
    "        print('\\rCould not find label for image of ' + storm_name + ' at date ' + str(date) + ' and time ' + str(time), end='\\n')\n",
    "        continue  # Skip to the next hurricane image if the a wind speed could not be found for this hurricane image\n",
    "\n",
    "    # Add the image and wind speed to these lists. This way, the lists of images and labels always line up. The first\n",
    "    # hurricane image in the images list is associated with the first wind speed in the labels list.\n",
    "    images.append(cropped_ir_data)\n",
    "    labels.append(wind_speed)\n",
    "\n",
    "    raw_data.close()\n",
    "\n",
    "    print('\\rProcessing Samples... ' + str(round(((i + 1) / num_files) * 100, 1)) + '% (' + str(i + 1) + ' of ' + str(num_files) + ')', end='')\n",
    "\n",
    "print('\\nSaving NumPy arrays...')\n",
    "\n",
    "# Turn the list of images and labels into NumPy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Add a fourth dimension to the images array. This is one since we only have one color channel: grayscale. The fourth\n",
    "# dimension would typically be 3 if we were working with color images\n",
    "images = images.reshape((images.shape[0], side_length, side_length, 1))\n",
    "\n",
    "# Save the NumPy arrays for use in model.py, where the neural network is trained and validated on this data\n",
    "np.save('/content/drive/MyDrive/hursat/NI_NA_images.npy', images)\n",
    "np.save('/content/drive/MyDrive/hursat/NI_NA_labels.npy', labels)\n",
    "\n",
    "print(\"\\nNumPy files saved. Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7-ONytX1lyUc",
   "metadata": {
    "id": "7-ONytX1lyUc"
   },
   "outputs": [],
   "source": [
    "# Outline:\n",
    "# - Reads numpy files containing hurricane images and the wind speed they are associated with\n",
    "# - Shows 10 random satellite images and their wind speeds\n",
    "# - When running the script, closing the current matplotlib window will cause the next one to open\n",
    "\n",
    "images = np.load('/content/drive/MyDrive/hursat/NI_NA_images.npy')\n",
    "labels = np.load('/content/drive/MyDrive/hursat/NI_NA_labels.npy')\n",
    "\n",
    "for x in range(10):\n",
    "    i = random.randint(0, images.shape[0])\n",
    "    image = np.reshape(images[i], (images[i].shape[0], images[i].shape[1]))\n",
    "    plt.imshow(image, cmap='binary')\n",
    "    plt.title('Image #' + str(i) + '   ' + str(labels[i]) + ' knots')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bDSHK7oRm1x0",
   "metadata": {
    "id": "bDSHK7oRm1x0"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Specify whether the script should use Keras's ImageDataGenerator to augment the training dataset. Assigning\n",
    "    # this variable to True will improve accuracy, but will also increase execution time.\n",
    "    AUGMENT = True\n",
    "\n",
    "    # Specify how many folds in the k-fold validation process. Can be any integer greater than or equal to 2. Larger\n",
    "    # integers will increase execution time.\n",
    "    NUM_FOLDS = 5\n",
    "\n",
    "    train_images, train_labels, test_images, test_labels = read_and_prepare_data('k_fold', NUM_FOLDS, augment=AUGMENT)\n",
    "    model = build_model()\n",
    "    predictions = pd.DataFrame(columns=['prediction', 'actual', 'abs_error', 'category'])\n",
    "    for i in range(NUM_FOLDS):\n",
    "        print('\\n\\nTraining Fold ' + str(i + 1) + ' of ' + str(NUM_FOLDS) + '\\n')\n",
    "        model = train_model(model, train_images[i], train_labels[i], test_images[i], test_labels[i])\n",
    "        kth_fold_predictions = generate_predictions(model, test_images[i], test_labels[i])\n",
    "        predictions = predictions.append(kth_fold_predictions, ignore_index = True)\n",
    "    show_validation_results(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3k03e_2eUyDs",
   "metadata": {
    "id": "3k03e_2eUyDs"
   },
   "source": [
    "# **Testing NA model on NI dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q74F9Q5R_KrC",
   "metadata": {
    "id": "Q74F9Q5R_KrC"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i6NMQqcoOQaP",
   "metadata": {
    "id": "i6NMQqcoOQaP"
   },
   "outputs": [],
   "source": [
    "model = load_model('/content/drive/MyDrive/hursat/NA.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kTe4gnzjQzJe",
   "metadata": {
    "id": "kTe4gnzjQzJe"
   },
   "outputs": [],
   "source": [
    "csv_path = '/content/drive/MyDrive/hursat/ibtracs_NI.csv'\n",
    "image_path = '/content/drive/MyDrive/hursat/NI_images.npy'\n",
    "label_path = '/content/drive/MyDrive/hursat/NI_labels.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bDeKhVOFPbBx",
   "metadata": {
    "id": "bDeKhVOFPbBx"
   },
   "outputs": [],
   "source": [
    "def standardize_data(images):\n",
    "    images[images < 0] = 0\n",
    "    st_dev = np.std(images)\n",
    "    mean = np.mean(images)\n",
    "    images = np.divide(np.subtract(images, mean), st_dev)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cSyr6nUEO9i5",
   "metadata": {
    "id": "cSyr6nUEO9i5"
   },
   "outputs": [],
   "source": [
    "def read_and_prepare_data(k):\n",
    "    images = np.load(image_path)\n",
    "    labels = np.load(label_path)\n",
    "\n",
    "    folded_images = []\n",
    "    folded_labels = []\n",
    "    for i in range(k):\n",
    "        start = int((i / k) * len(images))\n",
    "        end = int(((i + 1) / k) * len(images))\n",
    "        folded_images.append(images[start:end])\n",
    "        folded_labels.append(labels[start:end])\n",
    "\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    for i in range(k):\n",
    "        test_images.append(folded_images[i])\n",
    "        test_labels.append(folded_labels[i])\n",
    "        test_images[i] = standardize_data(test_images[i])\n",
    "\n",
    "    return test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DuhLfBJw3yHc",
   "metadata": {
    "id": "DuhLfBJw3yHc"
   },
   "outputs": [],
   "source": [
    "def generate_predictions(model, images, labels):\n",
    "    raw_predictions = model.predict(images)\n",
    "    raw_predictions = raw_predictions.flatten()\n",
    "\n",
    "    processed_predictions = pd.DataFrame(columns = ['prediction', 'actual', 'abs_error', 'category'])\n",
    "    for i in range(len(raw_predictions)):\n",
    "        abs_error = abs(raw_predictions[i] - labels[i])\n",
    "        new_row = {'prediction': raw_predictions[i], 'actual': labels[i], 'abs_error': abs_error,\n",
    "                   'abs_error_squared': abs_error ** 2, 'category': category_of(labels[i])}\n",
    "        processed_predictions = processed_predictions.append(new_row, ignore_index = True)\n",
    "        print_progress('Processing Predictions', i + 1, len(raw_predictions))\n",
    "\n",
    "    return processed_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kg3IGThtDXCP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3144,
     "status": "ok",
     "timestamp": 1665160097744,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "kg3IGThtDXCP",
    "outputId": "8bc6b92c-f9fe-4184-8f22-622420347315"
   },
   "outputs": [],
   "source": [
    "num_folds = 1\n",
    "\n",
    "images, labels = read_and_prepare_data(num_folds)\n",
    "\n",
    "predictions = pd.DataFrame(columns = ['prediction', 'actual', 'abs_error', 'category'])\n",
    "\n",
    "for i in range(num_folds):\n",
    "    print('\\n\\nTesting Fold ' + str(i + 1) + ' of ' + str(num_folds) + '\\n')\n",
    "    kth_fold_predictions = generate_predictions(model, images[i], labels[i])\n",
    "    predictions = predictions.append(kth_fold_predictions, ignore_index = True)\n",
    "show_validation_results(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0O0xWSJqJGR1",
   "metadata": {
    "id": "0O0xWSJqJGR1"
   },
   "source": [
    "# **Testing NP model on NI dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CjdkgzF__okb",
   "metadata": {
    "id": "CjdkgzF__okb"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaZG755JJOeo",
   "metadata": {
    "id": "eaZG755JJOeo"
   },
   "outputs": [],
   "source": [
    "model = load_model('/content/drive/MyDrive/hursat/NP.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OGwoJ2R3SZU7",
   "metadata": {
    "id": "OGwoJ2R3SZU7"
   },
   "outputs": [],
   "source": [
    "csv_path = '/content/drive/MyDrive/hursat/ibtracs_NI.csv'\n",
    "image_path = '/content/drive/MyDrive/hursat/NI_images.npy'\n",
    "label_path = '/content/drive/MyDrive/hursat/NI_labels.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mWRvAkG4h2ou",
   "metadata": {
    "id": "mWRvAkG4h2ou"
   },
   "outputs": [],
   "source": [
    "def standardize_data(images):\n",
    "    images[images < 0] = 0\n",
    "    st_dev = np.std(images)\n",
    "    mean = np.mean(images)\n",
    "    images = np.divide(np.subtract(images, mean), st_dev)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wjZohRAAjDFY",
   "metadata": {
    "id": "wjZohRAAjDFY"
   },
   "outputs": [],
   "source": [
    "def read_and_prepare_data(k):\n",
    "    images = np.load(image_path)\n",
    "    labels = np.load(label_path)\n",
    "\n",
    "    folded_images = []\n",
    "    folded_labels = []\n",
    "    for i in range(k):\n",
    "        start = int((i / k) * len(images))\n",
    "        end = int(((i + 1) / k) * len(images))\n",
    "        folded_images.append(images[start:end])\n",
    "        folded_labels.append(labels[start:end])\n",
    "\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    for i in range(k):\n",
    "        test_images.append(folded_images[i])\n",
    "        test_labels.append(folded_labels[i])\n",
    "        test_images[i] = standardize_data(test_images[i])\n",
    "\n",
    "    return test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_1XN14qciXLk",
   "metadata": {
    "id": "_1XN14qciXLk"
   },
   "outputs": [],
   "source": [
    "def generate_predictions(model, images, labels):\n",
    "    raw_predictions = model.predict(images)\n",
    "    raw_predictions = raw_predictions.flatten()\n",
    "\n",
    "    processed_predictions = pd.DataFrame(columns = ['prediction', 'actual', 'abs_error', 'category'])\n",
    "    for i in range(len(raw_predictions)):\n",
    "        abs_error = abs(raw_predictions[i] - labels[i])\n",
    "        new_row = {'prediction': raw_predictions[i], 'actual': labels[i], 'abs_error': abs_error,\n",
    "                   'abs_error_squared': abs_error ** 2, 'category': category_of(labels[i])}\n",
    "        processed_predictions = processed_predictions.append(new_row, ignore_index = True)\n",
    "        print_progress('Processing Predictions', i + 1, len(raw_predictions))\n",
    "\n",
    "    return processed_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3VsMRzqFjGHF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "executionInfo": {
     "elapsed": 12572,
     "status": "ok",
     "timestamp": 1665160396951,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "3VsMRzqFjGHF",
    "outputId": "18f92375-1b57-41bb-c840-f239c9de39c4"
   },
   "outputs": [],
   "source": [
    "num_folds = 1\n",
    "\n",
    "test_images, test_labels = read_and_prepare_data(num_folds)\n",
    "\n",
    "predictions = pd.DataFrame(columns = ['prediction', 'actual', 'abs_error', 'category'])\n",
    "\n",
    "for i in range(num_folds):\n",
    "    print('\\n\\nTesting Fold ' + str(i + 1) + ' of ' + str(num_folds) + '\\n')\n",
    "    kth_fold_predictions = generate_predictions(model, test_images[i], test_labels[i])\n",
    "    predictions = predictions.append(kth_fold_predictions, ignore_index = True)\n",
    "show_validation_results(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m9UzXEXEaLiL",
   "metadata": {
    "id": "m9UzXEXEaLiL"
   },
   "source": [
    "# **Testing NP+NA model on NI dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kIgw_7TraUsq",
   "metadata": {
    "id": "kIgw_7TraUsq"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0mHqwohIakUL",
   "metadata": {
    "id": "0mHqwohIakUL"
   },
   "outputs": [],
   "source": [
    "model = load_model('/content/drive/MyDrive/hursat/NP_NA.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RXFjGvnKTV-a",
   "metadata": {
    "id": "RXFjGvnKTV-a"
   },
   "outputs": [],
   "source": [
    "csv_path = '/content/drive/MyDrive/hursat/ibtracs_NI.csv'\n",
    "image_path = '/content/drive/MyDrive/hursat/NI_images.npy'\n",
    "label_path = '/content/drive/MyDrive/hursat/NI_labels.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4yAu8mEScaej",
   "metadata": {
    "id": "4yAu8mEScaej"
   },
   "outputs": [],
   "source": [
    "def standardize_data(images):\n",
    "    images[images < 0] = 0\n",
    "    st_dev = np.std(images)\n",
    "    mean = np.mean(images)\n",
    "    images = np.divide(np.subtract(images, mean), st_dev)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S9dOhINRclJR",
   "metadata": {
    "id": "S9dOhINRclJR"
   },
   "outputs": [],
   "source": [
    "def read_and_prepare_data(k):\n",
    "    images = np.load(image_path)\n",
    "    labels = np.load(label_path)\n",
    "\n",
    "    folded_images = []\n",
    "    folded_labels = []\n",
    "    for i in range(k):\n",
    "        start = int((i / k) * len(images))\n",
    "        end = int(((i + 1) / k) * len(images))\n",
    "        folded_images.append(images[start:end])\n",
    "        folded_labels.append(labels[start:end])\n",
    "\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    for i in range(k):\n",
    "        test_images.append(folded_images[i])\n",
    "        test_labels.append(folded_labels[i])\n",
    "        test_images[i] = standardize_data(test_images[i])\n",
    "\n",
    "    return test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QyOCimincor7",
   "metadata": {
    "id": "QyOCimincor7"
   },
   "outputs": [],
   "source": [
    "def generate_predictions(model, images, labels):\n",
    "    raw_predictions = model.predict(images)\n",
    "    raw_predictions = raw_predictions.flatten()\n",
    "\n",
    "    processed_predictions = pd.DataFrame(columns = ['prediction', 'actual', 'abs_error', 'category'])\n",
    "    for i in range(len(raw_predictions)):\n",
    "        abs_error = abs(raw_predictions[i] - labels[i])\n",
    "        new_row = {'prediction': raw_predictions[i], 'actual': labels[i], 'abs_error': abs_error,\n",
    "                   'abs_error_squared': abs_error ** 2, 'category': category_of(labels[i])}\n",
    "        processed_predictions = processed_predictions.append(new_row, ignore_index = True)\n",
    "        print_progress('Processing Predictions', i + 1, len(raw_predictions))\n",
    "\n",
    "    return processed_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZeWIIBamcuWI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "executionInfo": {
     "elapsed": 3171,
     "status": "ok",
     "timestamp": 1665160592682,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "ZeWIIBamcuWI",
    "outputId": "3377a0f3-6697-4099-9bdd-e5ad79d1ef8c"
   },
   "outputs": [],
   "source": [
    "num_folds = 1\n",
    "\n",
    "test_images, test_labels = read_and_prepare_data(num_folds)\n",
    "\n",
    "predictions = pd.DataFrame(columns = ['prediction', 'actual', 'abs_error', 'category'])\n",
    "\n",
    "for i in range(num_folds):\n",
    "    print('\\n\\nTesting Fold ' + str(i + 1) + ' of ' + str(num_folds) + '\\n')\n",
    "    kth_fold_predictions = generate_predictions(model, test_images[i], test_labels[i])\n",
    "    predictions = predictions.append(kth_fold_predictions, ignore_index = True)\n",
    "show_validation_results(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3Ufk5qCmfotL",
   "metadata": {
    "id": "3Ufk5qCmfotL"
   },
   "source": [
    "# **North Pacific + North Indian + North Atlantic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d_2lmJmngKAa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 967945,
     "status": "ok",
     "timestamp": 1653289188385,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "d_2lmJmngKAa",
    "outputId": "f27adc1c-73da-4a8d-bff0-f514d241a621"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Specify a list of years. Satellite images of hurricanes from those years will be downloaded.\n",
    "    YEARS_TO_DOWNLOAD = ['2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016']\n",
    "    path = '/content/drive/MyDrive/ibtracs_NP_NI_NA.csv'\n",
    "    download_hursat(YEARS_TO_DOWNLOAD, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_p-nXSD1gRFv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 731425,
     "status": "ok",
     "timestamp": 1653289919804,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "_p-nXSD1gRFv",
    "outputId": "609da5d5-7f62-4c98-8b07-2a9b7613ef09"
   },
   "outputs": [],
   "source": [
    "# This file takes the satellite image files downloaded and turns them into NumPy arrays that will be fed to the neural network\n",
    "# Outline of this file:\n",
    "# - Crops each satellite image\n",
    "# - Matches each satellite image of a hurricane with the maximum sustained wind speed of that hurricane\n",
    "# - Collects these images and their associated labels in arrays and saves them as numpy files\n",
    "\n",
    "# Reads in the Best Track dataset, which contain records of the location and maximum wind speed of every recorded hurricane in the Atlantic and Eastern/Central Pacific basins\n",
    "best_track_data = pd.read_csv('/content/drive/MyDrive/ibtracs_NP_NI_NA.csv')\n",
    "\n",
    "# The number of pixels wide and tall to crop the images of hurricanes to\n",
    "side_length = 50\n",
    "\n",
    "# Lists to hold the hurricane images and the wind speed associated with those images. These lists are aligned so that\n",
    "# the first image in the images list corresponds to the first label in the labels list.\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Gets list of names of files, each file containing a satellite image\n",
    "files = os.listdir('Satellite Imagery')\n",
    "num_files = len(files)\n",
    "\n",
    "for i in range(len(files)):\n",
    "    # Get IR satellite image from the file\n",
    "    raw_data = netCDF4.Dataset('Satellite Imagery/' + files[i])\n",
    "    ir_data = raw_data.variables['IRWIN'][0]\n",
    "\n",
    "    # 'Crop' the image by removing north, south, east, and west edges\n",
    "    south_bound = (ir_data.shape[0] - side_length) // 2\n",
    "    north_bound = south_bound + side_length\n",
    "    cropped_ir_data = ir_data[south_bound:north_bound]\n",
    "    west_bound = (ir_data.shape[1] - side_length) // 2\n",
    "    east_bound = side_length\n",
    "    cropped_ir_data = np.delete(cropped_ir_data, np.s_[:west_bound], axis=1)\n",
    "    cropped_ir_data = np.delete(cropped_ir_data, np.s_[east_bound:], axis=1)\n",
    "\n",
    "    # Get storm name, date, and time of the hurricane from the image's file name\n",
    "    file_name = files[i]\n",
    "    file_name = file_name.split('.')\n",
    "    storm_name = file_name[1]\n",
    "    date = int(file_name[2] + file_name[3] + file_name[4])\n",
    "    time = int(file_name[5])\n",
    "\n",
    "    # Filter the best track dataset to find the row that matches the name, date, and time of this hurricane image\n",
    "    matching_best_track_data = best_track_data.loc[(best_track_data.storm_name == storm_name) & (best_track_data.fulldate == date) & (best_track_data.time == time)]\n",
    "\n",
    "    # Get the wind speed from the row that matches the name, date, and time of this hurricane image\n",
    "    try:\n",
    "        wind_speed = matching_best_track_data.max_sus_wind_speed.reset_index(drop = True)[0]\n",
    "    except Exception:\n",
    "        print('\\rCould not find label for image of ' + storm_name + ' at date ' + str(date) + ' and time ' + str(time), end='\\n')\n",
    "        continue  # Skip to the next hurricane image if the a wind speed could not be found for this hurricane image\n",
    "\n",
    "    # Add the image and wind speed to these lists. This way, the lists of images and labels always line up. The first\n",
    "    # hurricane image in the images list is associated with the first wind speed in the labels list.\n",
    "    images.append(cropped_ir_data)\n",
    "    labels.append(wind_speed)\n",
    "\n",
    "    raw_data.close()\n",
    "\n",
    "    print('\\rProcessing Samples... ' + str(round(((i + 1) / num_files) * 100, 1)) + '% (' + str(i + 1) + ' of ' + str(num_files) + ')', end='')\n",
    "\n",
    "print('\\nSaving NumPy arrays...')\n",
    "\n",
    "# Turn the list of images and labels into NumPy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Add a fourth dimension to the images array. This is one since we only have one color channel: grayscale. The fourth\n",
    "# dimension would typically be 3 if we were working with color images\n",
    "images = images.reshape((images.shape[0], side_length, side_length, 1))\n",
    "\n",
    "# Save the NumPy arrays for use in model.py, where the neural network is trained and validated on this data\n",
    "np.save('images.npy', images)\n",
    "np.save('labels.npy', labels)\n",
    "\n",
    "print(\"\\nNumPy files saved. Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-BxCN2F1giXu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2439,
     "status": "ok",
     "timestamp": 1653289922239,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "-BxCN2F1giXu",
    "outputId": "59c4dd8a-eb0e-4b47-b3db-8485b0773aec"
   },
   "outputs": [],
   "source": [
    "# Outline:\n",
    "# - Reads numpy files containing hurricane images and the wind speed they are associated with\n",
    "# - Shows 10 random satellite images and their wind speeds\n",
    "# - When running the script, closing the current matplotlib window will cause the next one to open\n",
    "\n",
    "images = np.load('images.npy')\n",
    "labels = np.load('labels.npy')\n",
    "\n",
    "for x in range(5):\n",
    "    i = random.randint(0, images.shape[0])\n",
    "    image = np.reshape(images[i], (images[i].shape[0], images[i].shape[1]))\n",
    "    plt.imshow(image, cmap='binary')\n",
    "    plt.title('Image #' + str(i) + '   ' + str(labels[i]) + ' knots')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1zsZsOwFhsFd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4437788,
     "status": "ok",
     "timestamp": 1653294361671,
     "user": {
      "displayName": "Manish Mawatwal",
      "userId": "17196796811908430118"
     },
     "user_tz": -330
    },
    "id": "1zsZsOwFhsFd",
    "outputId": "68771f3c-5aa4-4712-c172-047f0d55fb14"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Specify whether the script should use Keras's ImageDataGenerator to augment the training dataset. Assigning\n",
    "    # this variable to True will improve accuracy, but will also increase execution time.\n",
    "    AUGMENT = True\n",
    "\n",
    "    # Specify how many folds in the k-fold validation process. Can be any integer greater than or equal to 2. Larger\n",
    "    # integers will increase execution time.\n",
    "    NUM_FOLDS = 5\n",
    "\n",
    "    train_images, train_labels, test_images, test_labels = read_and_prepare_data('k_fold', NUM_FOLDS, augment=AUGMENT)\n",
    "    model = build_model()\n",
    "    predictions = pd.DataFrame(columns=['prediction', 'actual', 'abs_error', 'category'])\n",
    "    for i in range(NUM_FOLDS):\n",
    "        print('\\n\\nTraining Fold ' + str(i + 1) + ' of ' + str(NUM_FOLDS) + '\\n')\n",
    "        model = train_model(model, train_images[i], train_labels[i], test_images[i], test_labels[i])\n",
    "        kth_fold_predictions = generate_predictions(model, test_images[i], test_labels[i])\n",
    "        predictions = predictions.append(kth_fold_predictions, ignore_index=True)\n",
    "    show_validation_results(predictions)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "M5WlEGKkXSLZ",
    "sxTPnrcHTHjH",
    "CH9dn4YpEbLl",
    "wTSnjfCBZYE7",
    "6z-GI-2TdtuN",
    "3k03e_2eUyDs",
    "0O0xWSJqJGR1",
    "m9UzXEXEaLiL",
    "3Ufk5qCmfotL"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
